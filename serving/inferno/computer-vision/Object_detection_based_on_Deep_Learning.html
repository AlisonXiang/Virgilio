<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Object Detection based on Deep Learning | Virgilio</title>
    <meta name="generator" content="VuePress 1.4.1">
    
    <meta name="description" content="Data Science E-Learning">
    <link rel="preload" href="/Virgilio/assets/css/0.styles.841bceba.css" as="style"><link rel="preload" href="/Virgilio/assets/js/app.c260f7e6.js" as="script"><link rel="preload" href="/Virgilio/assets/js/2.bd83452f.js" as="script"><link rel="preload" href="/Virgilio/assets/js/3.b67ff6cb.js" as="script"><link rel="preload" href="/Virgilio/assets/js/42.05198984.js" as="script"><link rel="prefetch" href="/Virgilio/assets/js/10.0ab9ed7e.js"><link rel="prefetch" href="/Virgilio/assets/js/11.0e8dc1d6.js"><link rel="prefetch" href="/Virgilio/assets/js/12.40d52791.js"><link rel="prefetch" href="/Virgilio/assets/js/13.20e24eae.js"><link rel="prefetch" href="/Virgilio/assets/js/14.44b706c7.js"><link rel="prefetch" href="/Virgilio/assets/js/15.b3c4c734.js"><link rel="prefetch" href="/Virgilio/assets/js/16.5e61c0c0.js"><link rel="prefetch" href="/Virgilio/assets/js/17.27233bde.js"><link rel="prefetch" href="/Virgilio/assets/js/18.c67b7b2b.js"><link rel="prefetch" href="/Virgilio/assets/js/19.bbd0654b.js"><link rel="prefetch" href="/Virgilio/assets/js/20.412db73b.js"><link rel="prefetch" href="/Virgilio/assets/js/21.2db607aa.js"><link rel="prefetch" href="/Virgilio/assets/js/22.0b4ee268.js"><link rel="prefetch" href="/Virgilio/assets/js/23.a2a899b4.js"><link rel="prefetch" href="/Virgilio/assets/js/24.24ceb642.js"><link rel="prefetch" href="/Virgilio/assets/js/25.465928a8.js"><link rel="prefetch" href="/Virgilio/assets/js/26.5d01e2bd.js"><link rel="prefetch" href="/Virgilio/assets/js/27.8c7d6526.js"><link rel="prefetch" href="/Virgilio/assets/js/28.b6783443.js"><link rel="prefetch" href="/Virgilio/assets/js/29.2ed61ffa.js"><link rel="prefetch" href="/Virgilio/assets/js/30.b30e1b07.js"><link rel="prefetch" href="/Virgilio/assets/js/31.e1090693.js"><link rel="prefetch" href="/Virgilio/assets/js/32.69971771.js"><link rel="prefetch" href="/Virgilio/assets/js/33.41def5ee.js"><link rel="prefetch" href="/Virgilio/assets/js/34.8656d523.js"><link rel="prefetch" href="/Virgilio/assets/js/35.e9698970.js"><link rel="prefetch" href="/Virgilio/assets/js/36.3dd02c04.js"><link rel="prefetch" href="/Virgilio/assets/js/37.91df2866.js"><link rel="prefetch" href="/Virgilio/assets/js/38.f35ef104.js"><link rel="prefetch" href="/Virgilio/assets/js/39.b566b923.js"><link rel="prefetch" href="/Virgilio/assets/js/4.ed651573.js"><link rel="prefetch" href="/Virgilio/assets/js/40.8ccc2317.js"><link rel="prefetch" href="/Virgilio/assets/js/41.3d0d98c8.js"><link rel="prefetch" href="/Virgilio/assets/js/43.2d8df4cd.js"><link rel="prefetch" href="/Virgilio/assets/js/44.e57fb78b.js"><link rel="prefetch" href="/Virgilio/assets/js/45.f1f2a7ac.js"><link rel="prefetch" href="/Virgilio/assets/js/46.6dddfa2b.js"><link rel="prefetch" href="/Virgilio/assets/js/47.34e47505.js"><link rel="prefetch" href="/Virgilio/assets/js/48.2b305efe.js"><link rel="prefetch" href="/Virgilio/assets/js/49.e94ca32c.js"><link rel="prefetch" href="/Virgilio/assets/js/5.f76cf809.js"><link rel="prefetch" href="/Virgilio/assets/js/50.2308b252.js"><link rel="prefetch" href="/Virgilio/assets/js/51.135c3834.js"><link rel="prefetch" href="/Virgilio/assets/js/52.74054f28.js"><link rel="prefetch" href="/Virgilio/assets/js/53.0e061a9b.js"><link rel="prefetch" href="/Virgilio/assets/js/54.34303d29.js"><link rel="prefetch" href="/Virgilio/assets/js/55.ae054be4.js"><link rel="prefetch" href="/Virgilio/assets/js/56.f4257664.js"><link rel="prefetch" href="/Virgilio/assets/js/57.8fd3acdc.js"><link rel="prefetch" href="/Virgilio/assets/js/58.33054660.js"><link rel="prefetch" href="/Virgilio/assets/js/59.4680d4fc.js"><link rel="prefetch" href="/Virgilio/assets/js/6.212bec39.js"><link rel="prefetch" href="/Virgilio/assets/js/60.75171d7c.js"><link rel="prefetch" href="/Virgilio/assets/js/61.222e2490.js"><link rel="prefetch" href="/Virgilio/assets/js/62.9f007d69.js"><link rel="prefetch" href="/Virgilio/assets/js/63.66c5454f.js"><link rel="prefetch" href="/Virgilio/assets/js/64.db67d449.js"><link rel="prefetch" href="/Virgilio/assets/js/65.31ea8ac2.js"><link rel="prefetch" href="/Virgilio/assets/js/66.ab1e7d82.js"><link rel="prefetch" href="/Virgilio/assets/js/67.d26116e6.js"><link rel="prefetch" href="/Virgilio/assets/js/68.2d68a15c.js"><link rel="prefetch" href="/Virgilio/assets/js/69.1ded4df8.js"><link rel="prefetch" href="/Virgilio/assets/js/7.813d6358.js"><link rel="prefetch" href="/Virgilio/assets/js/70.19a36114.js"><link rel="prefetch" href="/Virgilio/assets/js/71.6d9cfe1f.js"><link rel="prefetch" href="/Virgilio/assets/js/72.f901ab53.js"><link rel="prefetch" href="/Virgilio/assets/js/73.2635fcce.js"><link rel="prefetch" href="/Virgilio/assets/js/74.459fa34e.js"><link rel="prefetch" href="/Virgilio/assets/js/75.beed7aff.js"><link rel="prefetch" href="/Virgilio/assets/js/76.ce27a124.js"><link rel="prefetch" href="/Virgilio/assets/js/77.338fa8dd.js"><link rel="prefetch" href="/Virgilio/assets/js/78.d2ea2a32.js"><link rel="prefetch" href="/Virgilio/assets/js/8.b1cf2d78.js"><link rel="prefetch" href="/Virgilio/assets/js/9.5113a612.js">
    <link rel="stylesheet" href="/Virgilio/assets/css/0.styles.841bceba.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/Virgilio/" class="home-link router-link-active"><!----> <span class="site-name">Virgilio <span class="site-name2"> Data Science </span></span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="https://github.com/virgili0/Virgilio" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Contribute
  <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="https://github.com/virgili0/Virgilio" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Contribute
  <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></div> <!----></nav>  <ul class="sidebar-links"><li><a href="/Virgilio/" class="sidebar-link">What is Virgilio?</a></li><li><section class="sidebar-group depth-0"><p class="sidebar-heading"><span>Paradiso</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/Virgilio/serving/paradiso/demystification-ai-ml-dl.html" class="sidebar-link">Demystification of the key concepts of AI and ML</a></li><li><a href="/Virgilio/serving/paradiso/what-do-i-need-for-ml.html" class="sidebar-link">What do I need to do Machine Learning?</a></li><li><a href="/Virgilio/serving/paradiso/do-you-really-need-ml.html" class="sidebar-link">Do you really need Machine Learning?</a></li><li><a href="/Virgilio/serving/paradiso/use-cases.html" class="sidebar-link">Machine Learning use cases</a></li><li><a href="/Virgilio/serving/paradiso/virgilio-teaching-strategy.html" class="sidebar-link">Learning to Learn</a></li><li><a href="/Virgilio/serving/paradiso/introduction-to-ml.html" class="sidebar-link">Introduction to Machine Learning</a></li></ul></section></li><li><section class="sidebar-group depth-0"><p class="sidebar-heading"><span>Purgatorio</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><section class="sidebar-group is-sub-group depth-1"><p class="sidebar-heading open"><span>Fundamentals</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/Virgilio/serving/purgatorio/fundamentals/math-fundamentals.html" class="sidebar-link">Linear Algebra</a></li><li><a href="/Virgilio/serving/purgatorio/fundamentals/statistics-fundamentals.html" class="sidebar-link">Statistics</a></li><li><a href="/Virgilio/serving/purgatorio/fundamentals/python-fundamentals.html" class="sidebar-link">Python</a></li><li><a href="/Virgilio/serving/purgatorio/fundamentals/jupyter-notebook.html" class="sidebar-link">Jupyter Notebook</a></li><li><a href="/Virgilio/serving/purgatorio/fundamentals/the-data-science-process.html" class="sidebar-link">The Data Science Process</a></li></ul></section></li><li><section class="sidebar-group is-sub-group depth-1"><p class="sidebar-heading"><span>Define The Scope and Ask Questions</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/Virgilio/serving/purgatorio/define-the-scope-and-ask-questions/frame-the-problem.html" class="sidebar-link">Frame the Problem</a></li><li><a href="/Virgilio/serving/purgatorio/define-the-scope-and-ask-questions/usage-and-integration.html" class="sidebar-link">Usage and Integration</a></li><li><a href="/Virgilio/serving/purgatorio/define-the-scope-and-ask-questions/starting-a-data-project.html" class="sidebar-link">Starting a Data Project</a></li><li><a href="/Virgilio/serving/purgatorio/define-the-scope-and-ask-questions/workspace-setup-and-cloud-computing.html" class="sidebar-link">Workspace Setup and Cloud Computing</a></li></ul></section></li><li><section class="sidebar-group is-sub-group depth-1"><p class="sidebar-heading"><span>Collect and Prepare Data</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/Virgilio/serving/purgatorio/collect-and-prepare-data/data-preparation.html" class="sidebar-link">Data Preparation</a></li><li><a href="/Virgilio/serving/purgatorio/collect-and-prepare-data/data-visualization.html" class="sidebar-link">Data Visualization</a></li></ul></section></li><li><section class="sidebar-group is-sub-group depth-1"><p class="sidebar-heading"><span>Select and Train Machine Learning Models</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/Virgilio/serving/purgatorio/select-and-train-machine-learning-models/machine-learning-theory.html" class="sidebar-link">Machine Learning Theory</a></li><li><a href="/Virgilio/serving/purgatorio/select-and-train-machine-learning-models/deep-learning-theory.html" class="sidebar-link">Deep Learning Theory</a></li><li><a href="/Virgilio/serving/purgatorio/select-and-train-machine-learning-models/evaluation-and-finetuning.html" class="sidebar-link">Evaluation and Fine Tuning</a></li><li><a href="/Virgilio/serving/purgatorio/select-and-train-machine-learning-models/tools-and-libraries.html" class="sidebar-link">Tools and Libraries</a></li></ul></section></li><li><section class="sidebar-group is-sub-group depth-1"><p class="sidebar-heading"><span>Launch and Mantain the System </span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/Virgilio/serving/purgatorio/launch-and-mantain-the-system/serving-trained-models.html" class="sidebar-link">Serving Trained Models</a></li></ul></section></li></ul></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="object-detection-based-on-deep-learning"><a href="#object-detection-based-on-deep-learning" class="header-anchor">#</a> Object Detection based on Deep Learning</h1> <p><img src="https://i.ibb.co/GVbs5bV/fig.png" alt="DL"></p> <h2 id="what-is-object-detection"><a href="#what-is-object-detection" class="header-anchor">#</a> What is object detection?</h2> <p>In recent years, object detection in images has been greatly improved through numerous implementations of the Deep Learning paradigm. Creating trainable models helps to rapidly detect and classify discriminating features without having to develop a sophisticated algorithm. In fact, detecting objects and locating them in an image is an increasingly vital aspect of computer vision research. This discipline seeks out instances, their class labels and their positions in the visual data. This domain stands at the overlap of two other fields: image classification and object localization. Indeed, object detection is based on the following principle: for a specific image, we look for regions of the image that might contain an object and then, for each of these detected regions, we extract and classify it using an image classification model. Those regions of the initial image showing good classification results are maintained and the remainder is discarded. Thus, for a good object detection method, it is necessary to have a robust region detection algorithm as well as a good image classification model.</p> <p>The following projects and tutorials related to the video detection task are provided for further details:</p> <ul><li><p><a href="https://github.com/lbeaucourt/Object-detection" target="_blank" rel="noopener noreferrer">Object detection project for real-time (webcam) and offline (video processing) application.<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p></li> <li><p><a href="https://github.com/hjptriplebee/SSD_tensorflow" target="_blank" rel="noopener noreferrer">Object-detection: Single Shot MultiBox Detector(SSD) in TensorFlow.<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p></li> <li><p><a href="https://github.com/rlan/darknet" target="_blank" rel="noopener noreferrer">Darknet<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p></li> <li><p><a href="https://github.com/cedrickchee/pytorch-RetinaNet" target="_blank" rel="noopener noreferrer">PyTorch implementation of RetinaNet with the goal to reproduce results in the &quot;focal loss for dense object detection&quot; paper.<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p></li> <li><p><a href="https://github.com/ogbanugot/Robosapien-Object-Detector-using-Darkflow" target="_blank" rel="noopener noreferrer">Robosapien object detector using Darkflow<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p></li> <li><p><a href="https://github.com/siddartha19/Object-Detection-Application" target="_blank" rel="noopener noreferrer">Object Detection Application<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p></li> <li><p><a href="https://towardsdatascience.com/understanding-object-detection-9ba089154df8" target="_blank" rel="noopener noreferrer">Understanding Object Detection<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p></li></ul> <p>The following are some useful books that will help you learn the different aspects of object detection:</p> <ol><li><a href="https://aax-us-east.amazon-adsystem.com/x/c/Qja79-QaoJHEO-MVQ1oDZwsAAAFwtKQVSwEAAAFKAQDMYqQ/https://assoc-redirect.amazon.com/g/r/https://www.amazon.com/Advanced-Applied-Deep-Learning-Convolutional/dp/1484249755?creativeASIN=1484249755&amp;linkCode=w61&amp;imprToken=bnWxJoN9ehtfoAkba-1ChA&amp;slotNum=3&amp;tag=uuid10-20" target="_blank" rel="noopener noreferrer">Advanced Applied Deep Learning<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></li> <li><a href="https://aax-us-east.amazon-adsystem.com/x/c/Qja79-QaoJHEO-MVQ1oDZwsAAAFwtKQVSwEAAAFKAQDMYqQ/https://assoc-redirect.amazon.com/g/r/https://www.amazon.com/Detection-Low-spatial-resolution-Imagery-Convolutional-Networks/dp/1688093427?creativeASIN=1688093427&amp;linkCode=w61&amp;imprToken=bnWxJoN9ehtfoAkba-1ChA&amp;slotNum=9&amp;tag=uuid10-20" target="_blank" rel="noopener noreferrer">Object Detection in Low-spatial-resolution Aerial Imagery Using Convolutional Neural Networks<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></li> <li><a href="https://aax-us-east.amazon-adsystem.com/x/c/Qja79-QaoJHEO-MVQ1oDZwsAAAFwtKQVSwEAAAFKAQDMYqQ/https://assoc-redirect.amazon.com/g/r/https://www.amazon.com/Hierarchical-approach-object-detection-descriptors/dp/3330353066?creativeASIN=3330353066&amp;linkCode=w61&amp;imprToken=bnWxJoN9ehtfoAkba-1ChA&amp;slotNum=15&amp;tag=uuid10-20" target="_blank" rel="noopener noreferrer">Hierarchical approach for object detection using shape descriptors<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></li> <li><a href="https://aax-us-east.amazon-adsystem.com/x/c/Qja79-QaoJHEO-MVQ1oDZwsAAAFwtKQVSwEAAAFKAQDMYqQ/https://assoc-redirect.amazon.com/g/r/https://www.amazon.com/Application-Deep-Learning-Object-Detection/dp/613945705X?creativeASIN=613945705X&amp;linkCode=w61&amp;imprToken=bnWxJoN9ehtfoAkba-1ChA&amp;slotNum=21&amp;tag=uuid10-20" target="_blank" rel="noopener noreferrer">Application of Deep Learning in Object Detection
Application of Deep Learning in Object Detection using Tensorflow<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></li></ol> <h3 id="object-localization-v-s-object-classification"><a href="#object-localization-v-s-object-classification" class="header-anchor">#</a> Object localization v.s. object classification</h3> <p>Object detection consists of identifying and locating one or several objects in the image. Depending on a given input, a detector will return information of two dimensions: the class labels and location of each instance. Locating an object in an image is complex and measuring the localization performance needs an adapted metric. Moreover, unlike classification, several targets can be located in the same image and the detector must be able to accurately locate them. For a single input, an object detector returns the detected objects in the image and their associated bounding boxes. Some strategies are available to address this challenge. The first one(see Figure 1), based on object recognition approaches, essentially consists of simply predicting the size of a bounding box and the class to which it belongs. The second approach (see Figure 2), probably the best known, is the proposed region approach where another model extracts a reduced number of candidate frames (i.e. proposals) that contain an object and the problem is then simplified into a recognition problem.</p> <p><strong>Figure 1:</strong> Object detection based on a single-stage method <img src="https://i.ibb.co/fCjNBpw/1.png" alt="Fig1"></p> <p><strong>Figure 2:</strong> Object detection based on a two-stages method (region proposals) <img src="https://i.ibb.co/1MkgbS4/2.png" alt="Fig2"></p> <h3 id="contour-based-object-detection"><a href="#contour-based-object-detection" class="header-anchor">#</a> Contour-based object detection</h3> <p>cIn this section, we will discuss different methods of contour detection and compare them with each other.</p> <p><strong>1. SOBEL detector</strong></p> <p>The Sobel detector is one of the methods we're looking at right now. Like most detectors, this one is based on calculating the gradients of the image at each point. Sobel's discrete method is based on multiplying the intensity matrix around the desired pixel by the following matrices, representing the &quot; mixture &quot; between filters derived according to x and y, and Gaussian filters that add importance to the nearest pixels.</p> <p><img src="https://i.ibb.co/6cQx0b0/3.png" alt="Sobel"></p> <p><strong>2. LAPLACIAN detector</strong></p> <p>An alternative method that has been explored is the use of the Laplacian. The principle is quite similar and is based on the second derivative of the intensity. Discretely, the Laplacian matrix is implemented by the product of the intensity matrices of the pixel contour with the following matrices :</p> <p>[0 1 0]</p> <p>[1 -4 1]</p> <p>[0 1 0]</p> <p><img src="https://i.ibb.co/M7L0nbr/4.png" alt="Lap"></p> <p><strong>3. CANNY detector</strong></p> <p>The Canny method uses a Gaussian filter and then derivation matrices along both axes to determine the magnitude and angle of the gradient. Finally, a hysteresis is applied to smooth out the most important edges.</p> <p><img src="https://i.ibb.co/3C8xHxX/5.png" alt="Canny"></p> <p><strong>4. PREWITT detector</strong></p> <p>This detector is pretty close to Sobel's. Concretely, it operates on the principle of gradient detection along the two major axes, in combination with an averaging filter.</p> <p><img src="https://i.ibb.co/XZLpZCk/6.png" alt="prewitt"></p> <h3 id="conventional-methods-for-object-detection-a-use-case"><a href="#conventional-methods-for-object-detection-a-use-case" class="header-anchor">#</a> Conventional methods for object detection: A use case</h3> <p>The objective of this use case is to develop and program algorithms to detect faces in images. To do this, one approach using <a href="https://www.cs.cmu.edu/~efros/courses/LBMV07/Papers/viola-cvpr-01.pdf" target="_blank" rel="noopener noreferrer">Viola-Jones' algorithm<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> will be tested.
The Viola-Jones technique (Haar Cascade Face Detector) is a method of detecting objects in images proposed by Paul Viola and Michael Jones in 2001 and widely used for face detection. This method detects objects by learning a classifier.</p> <p>The implementation of this detector is as follows:</p> <div class="language- extra-class"><pre class="language-text"><code>class HaarCascadeFaceDetector(AbstractSkinDetector):
    &quot;&quot;&quot;
    Face detector with the Viola Jones method
    &quot;&quot;&quot;
    METHOD_NAME = &quot;viola_jones&quot;

    def process(self):
        # greyscale image for haar cascades
        self.original = self.original.switch_color_space(&quot;GRAY&quot;)
        cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')

        # face detections
        faces = cascade.detectMultiScale(self.original.img, scaleFactor=1.1,
                                         minNeighbors=5, minSize=(30, 30),
                                         flags = cv2.cv.CV_HAAR_SCALE_IMAGE)

        if DEBUG:
            print(&quot;%s faces detected&quot; % len(faces))

        # Bounding boxes are drawn around the faces on the resulting image.
        for (x, y, w, h) in faces:
            cv2.rectangle(self.result.img, (x, y), (x + w, y + h), (255, 0, 0), 2)
        self.result.save()


def run_haar_cascade():
    &quot;&quot;&quot;
    Detects faces via haar cascades in all images.
    &quot;&quot;&quot;
    for img_name in chain(train_dataset(), test_dataset()):
        _ = HaarCascadeFaceDetector(img_name)


def benchmark(Detector, extra_args, use_test_data=True):
    &quot;&quot;&quot;
    Run a benchmark of the detector passed in arg
        &quot;&quot;&quot;
    print(&quot;Benchmark started for %s(%s)&quot; % (Detector.__name__, extra_args))

    true_positive_rates = []
    false_positive_rates = []

    for image_name in (test_dataset() if use_test_data else train_dataset()):
        detector = Detector(
            image_name, *extra_args) if extra_args else Detector(image_name)
        true_positive_rate, false_positive_rate = detector.rates()
        true_positive_rates.append(true_positive_rate)
        false_positive_rates.append(false_positive_rate)

    # tp: true positive / fp: false positive
    # avg: average     / std: standard deviation
    tp_avg = np.mean(true_positive_rates)
    tp_std = np.std(true_positive_rates)
    fp_avg = np.mean(false_positive_rates)
    fp_std = np.std(false_positive_rates)

    print(&quot;Benchmark finished for %s(%s)&quot; % (Detector.__name__, extra_args))

    return tp_avg, tp_std, fp_avg, fp_std

</code></pre></div><p>The method is implemented with Opencv's haar cascade. We tested a few different parameters for this detector. Finally, we get an optimal result (see Figure) with a slight scaling of the image (10%), a minimum detection size of 30 pixels by 30 pixels and a minimum number of neighbors for the detection to be valid of 5.</p> <p><img src="https://i.ibb.co/R3PhwjP/8.png" alt="res"></p> <p>###<strong>Deep CNN -based object detection</strong></p> <p>Convolutional neural networks (CNN) are particular deep neural network (DNN) structures since the basic operation has become a convolution rather than a matrix multiplication. These networks have been developed to take advantage of big data with a structure (spatial, temporal, ...) such as images, videos, etc. As shown in Figure 3, their functioning is straightforward and requires some convolution operations.  The output is obtained by convolving the input image by a fixed number of kernels. The output of a kernel is obtained by multiplying the current position of a sliding window (i.e. receptive field) applied to the image. Besides, pooling or subsampling layers are added to the basic operations. They allow to reduce the spatial size of the representation in this way, they allow controlling overfitting problems.</p> <p><strong>Figure 3:</strong> A basic structure of CNN<img src="https://miro.medium.com/max/1000/1*zNs_mYOAgHpt3WxbYa7fnw.png" alt="CNN"></p> <p>###<strong>Use case 2:</strong> CNN-based generic object detector</p> <p><strong>Deep Learning Framework:</strong> <a href="https://pytorch.org/" target="_blank" rel="noopener noreferrer">Pytorch<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> (An open source machine learning framework that accelerates the path from research prototyping to production deployment). Please follow the installation instructions on the framework's official <a href="https://pytorch.org/" target="_blank" rel="noopener noreferrer">website<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.</p> <p><strong>Architecture:</strong> The CNN model consists of three convolutional layers and two fully connected layers. Each convolutional layer uses a kernel of size 5 with a stride of 1 and rectified linear units, ReLU, are used as the activation function. After each of the first two convolutional layers, a max-pooling layer of size 2 with a stride of 2 is used. The network was trained over 50 epochs using the stochastic gradient descent (SGD) optimizer. For the first 30 epochs, the learning rate is set to 0.0001 and then it is updated to 0.00001.</p> <p>The CNN implementation is as follows:</p> <div class="language- extra-class"><pre class="language-text"><code>import torch.nn as nn
import torch.nn.functional as f
class cnn_model(nn.Module):

    def __init__(self):

        super(cnn_model, self).__init__()

        self.conv1 = nn.Conv2d(
            in_channels=1,
            out_channels=32,
            kernel_size=5,
            stride=1,
            padding=0
        )

        self.conv2 = nn.Conv2d(
            in_channels=32,
            out_channels=64,
            kernel_size=5,
            stride=1,
            padding=0
        )

        self.conv3 = nn.Conv2d(
            in_channels=64,
            out_channels=128,
            kernel_size=5,
            stride=1,
            padding=0
        )


        self.fc1 = nn.Linear(
            in_features=18*18*128,
            out_features=2046
        )

        self.fc2 = nn.Linear(
            in_features=2046,
            out_features=4
        )

    def forward(self, val):
        val = f.relu(self.conv1(val))
        val = f.max_pool2d(val, kernel_size=2, stride=2)
        val = f.relu(self.conv2(val))
        val = f.max_pool2d(val, kernel_size=2, stride=2)
        val = f.relu(self.conv3(val))
        val = val.view(-1, 18*18*128)
        val = f.dropout(f.relu(self.fc1(val)), p=0.5, training=self.training)
        val = self.fc2(val)

        return val
</code></pre></div><p>Source code to learn the model:</p> <div class="language- extra-class"><pre class="language-text"><code>import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import numpy as np
import pandas as pd
from functions import overlapScore


from cnn_model import *
from training_dataset import *

def train_model(net, dataloader, batchSize, lr, momentum):
    criterion = nn.MSELoss()
    optimization = optim.SGD(net.parameters(), lr=lr, momentum=momentum)
    scheduler = optim.lr_scheduler.StepLR(optimization, step_size=30, gamma=0.1)

    for epoch in range(50):

        scheduler.step()

        for i, data in enumerate(dataloader):
            optimization.zero_grad()

            inputs, labels = data

            inputs, labels = inputs.view(batchSize,1, 100, 100), labels.view(batchSize, 4)

            outputs = net(inputs)

            loss = criterion(outputs, labels)
            loss.backward()
            optimization.step()

            pbox = outputs.detach().numpy()
            gbox = labels.detach().numpy()
            score, _ = overlapScore(pbox, gbox)

            print('[epoch %5d, step: %d, loss: %f, Average Score = %f' % (epoch+1, i+1, loss.item(), score/batchSize))

    print('Finish Training')


if __name__ == '__main__':
    # Hyper parameters
    learning_rate = 0.0001
    momentum = 0.9
    batch = 100
    no_of_workers = 2
    shuffle = True


    trainingdataset = training_dataset()
    dataLoader = DataLoader(
        dataset=trainingdataset,
        batch_size=batch,
        shuffle=shuffle,
        num_workers=no_of_workers
    )

    model = cnn_model()
    model.train()

    train_model(model, dataLoader, batch,learning_rate, momentum)
    torch.save(model.state_dict(), './trained_CNN_model.pth')
</code></pre></div><p>Source code for handling the dataset (reading data samples):</p> <div class="language- extra-class"><pre class="language-text"><code>import numpy as np
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader

class training_dataset(Dataset):
    def __init__(self):
      # Training set and corresponding ground truth images
        trainX = np.asarray(pd.read_csv('./my_dataset/trainingData.csv', sep=',', header=None))
        trainY = np.asarray(pd.read_csv('./my_dataset/ground-truth.csv', sep=',', header=None))
        self.features_train = torch.Tensor(trainX)
        self.groundTruth_train = torch.Tensor(trainY)

        self.len = len(trainX)


    def __getitem__(self, item):
        return self.features_train[item], self.groundTruth_train[item]

    def __len__(self):
        return self.len
</code></pre></div><p>The following pseudo-code entails a function to calculate the overlap rate taking into account the ground truth boxes and the predicted bounding boxes.</p> <div class="language- extra-class"><pre class="language-text"><code>import numpy as np

def overlap(rect1, rect2):

    avgScore = 0
    scores = []

    for i, _ in enumerate(rects1):

        rect1 = rect1[i]
        rect2 = rect2[i]

        left = np.max((rect1[0], rect2[0]))
        right = np.min((rect1[0]+rect1[2], rect2[0]+rect2[2]))

        top = np.max((rect1[1], rect2[1]))
        bottom = np.min((rect1[1]+rect1[3], rect2[1]+rect2[3]))

        # area of intersection
        i = np.max((0, right-left))*np.max((0,bottom-top))

        # combined area of two rectangles
        u = rect1[2]*rect1[3] + rect2[2]*rect2[3] - i

        # return the overlap ratio
        # value is always between 0 and 1
        score = np.clip(i/u, 0, 1)
        avgScore += score
        scores.append(score)

    return avgScore, scores
</code></pre></div><p>###<strong>Real-time object detection and segmentation with Tensorflow</strong>
The objective of this use case is to explain the real-time object detection and segmentation by an example. To do this, we will develop a real-time segmentation application with a simple webcam. We will use the <a href="https://www.tensorflow.org/" target="_blank" rel="noopener noreferrer">Tensorflow framework<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>, the Mask RCNN network learned with the COCO dataset; which allows us to detect up to 100 different types of objects.</p> <p>First of all, let's make a list of our tools and libraries to install.</p> <ul><li>Linux Ubuntu</li> <li>Python</li> <li>Tensorflow</li> <li>Open CV</li></ul> <p>In order to check if your computer is ready, open a python3 console in the terminal by typing python3. Do the following imports:</p> <div class="language- extra-class"><pre class="language-text"><code>import cv2
import numpy as np
import tensorflow as tf
from object_detection.utils import label_map_util
from object_detection.utils import ops as utils_ops
from object_detection.utils import visualization_utils as vis_util
</code></pre></div><p>To make the model work, you need to load the network and its weights. You will find a list of all object detection models available with Tensorflow on this zoo model. For this tutorial, download mask_rcnn_resnet101_atrous_coco.</p> <p>We start with the last point: initialize the webcam. This allows two things: (1) to create the video stream and (2) to get the width and height of a frame of the stream in order to configure the output tensor of the mask.</p> <div class="language- extra-class"><pre class="language-text"><code># Init the video stream (with the first plugged webcam)
cap = cv2.VideoCapture(0)
if cap.isOpened():
  # get vcap property
  global_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
  global_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
</code></pre></div><p>The next step is to import the label_map file. It is done using methods delivered with Tensorflow's object_detection.</p> <div class="language- extra-class"><pre class="language-text"><code>label_map = label_map_util.load_labelmap(&quot;PATH/TO/LABELS&quot;)
categories = label_map_util.convert_label_map_to_categories(
  label_map, 
  max_num_classes=NUM_CLASSES,
  use_display_name=True)
category_index = label_map_util.create_category_index(categories)
</code></pre></div><p>We then proceed with the import of the model and the collection of the tensors.</p> <div class="language- extra-class"><pre class="language-text"><code># Init TF Graph and get all needed tensors
detection_graph = tf.Graph()
with detection_graph.as_default():
  # Init the graph
  od_graph_def = tf.GraphDef()
  with tf.gfile.GFile(&quot;/chemin/vers/*.pbb') as fid:
    serialized_graph = fid.read()
    od_graph_def.ParseFromString(serialized_graph)
    tf.import_graph_def(od_graph_def, name='')

  # Get all tensors
  ops = tf.get_default_graph().get_operations()
  all_tensor_names = {output.name for op in ops for output in op.outputs}
  tensor_dict = {}
  for key in ['num_detections', 'detection_boxes', 'detection_scores', 'detection_classes', 'detection_masks']:
    tensor_name = key + ':0'
      if tensor_name in all_tensor_names:
        tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(tensor_name)

  # detection_masks tensor need ops
  detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])
  detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])
  # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.
  real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)
  detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])
  detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])
  detection_masks_reframed =  utils_ops.reframe_box_masks_to_image_masks(
detection_masks, detection_boxes, global_height, global_width)
  detection_masks_reframed = tf.cast(tf.greater(detection_masks_reframed, 0.5), tf.uint8)
  # Follow the convention by adding back the batch dimension
  tensor_dict['detection_masks'] = tf.expand_dims(detection_masks_reframed, 0)

  image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')
</code></pre></div><p>Let's continue our program with the main method. In this method which is called after the initializations we just presented, we capture the last image of the webcam, we process it, we display the modified image and we start again. So the main method works like this:</p> <div class="language- extra-class"><pre class="language-text"><code>if __name__ == '__main__':
  # Do that here and save a lot of time
  with detection_graph.as_default():
    with tf.Session(graph=detection_graph) as sess:
      while True:
        # Get the last frame
        frame = cap.read()[1]
        # Process last img
        new_frame = detect_objects(image_np, sess)
        # Display the resulting frame
        cv2.imshow('new_frame', new_frame)
        if cv2.waitKey(1) &amp; 0xFF == ord('q'):
          break
</code></pre></div><p>And so the core of the program is in the detect_objects() method which takes as arguments the image (in NumPy array format) and the Tensorflow session which will allow us to run our model.</p> <div class="language- extra-class"><pre class="language-text"><code>def detect_objects(image_np, sess):
  # Run inference
  output_dict = sess.run(tensor_dict, feed_dict={image_tensor: np.expand_dims(image_np, 0)})

  # all outputs are float32 numpy arrays, so convert types as appropriate
  output_dict['num_detections'] = int(output_dict['num_detections'][0])
  output_dict['detection_classes'] = output_dict['detection_classes'][0].astype(np.uint8)
  output_dict['detection_boxes'] = output_dict['detection_boxes'][0]
  output_dict['detection_scores'] = output_dict['detection_scores'][0]
  output_dict['detection_masks'] = output_dict['detection_masks'][0]

  # Display boxes and color pixels
  vis_util.visualize_boxes_and_labels_on_image_array(
    image_np,
    output_dict['detection_boxes'],
    output_dict['detection_classes'],
    output_dict['detection_scores'],
    category_index,
    instance_masks=output_dict.get('detection_masks    use_normalized_coordinates=True)

  return image_np
</code></pre></div></div> <footer class="page-edit"><!----> <!----></footer> <!----> </main></div><div class="global-ui"></div></div>
    <script src="/Virgilio/assets/js/app.c260f7e6.js" defer></script><script src="/Virgilio/assets/js/2.bd83452f.js" defer></script><script src="/Virgilio/assets/js/3.b67ff6cb.js" defer></script><script src="/Virgilio/assets/js/42.05198984.js" defer></script>
  </body>
</html>
