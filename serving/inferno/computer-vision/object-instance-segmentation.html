<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Object Instance Segmentation using TensorFlow Framework and Cloud GPU Technology</title>
    <meta name="generator" content="VuePress 1.4.1">
    
    <meta name="description" content="">
    <link rel="preload" href="/assets/css/0.styles.6e148dcf.css" as="style"><link rel="preload" href="/assets/js/app.124452e9.js" as="script"><link rel="preload" href="/assets/js/2.7c027d2f.js" as="script"><link rel="preload" href="/assets/js/44.e37bb31e.js" as="script"><link rel="prefetch" href="/assets/js/10.01c35a5d.js"><link rel="prefetch" href="/assets/js/11.0a8eb064.js"><link rel="prefetch" href="/assets/js/12.2b15887b.js"><link rel="prefetch" href="/assets/js/13.bcffe778.js"><link rel="prefetch" href="/assets/js/14.268f9c6d.js"><link rel="prefetch" href="/assets/js/15.adfcfd77.js"><link rel="prefetch" href="/assets/js/16.71d0f7e1.js"><link rel="prefetch" href="/assets/js/17.db7a9a8c.js"><link rel="prefetch" href="/assets/js/18.9820153b.js"><link rel="prefetch" href="/assets/js/19.72f304aa.js"><link rel="prefetch" href="/assets/js/20.c9e8a6e9.js"><link rel="prefetch" href="/assets/js/21.9fc19f36.js"><link rel="prefetch" href="/assets/js/22.8b628dc0.js"><link rel="prefetch" href="/assets/js/23.fc67d676.js"><link rel="prefetch" href="/assets/js/24.aca476d2.js"><link rel="prefetch" href="/assets/js/25.b39f1d23.js"><link rel="prefetch" href="/assets/js/26.57c9dd7f.js"><link rel="prefetch" href="/assets/js/27.7e36117f.js"><link rel="prefetch" href="/assets/js/28.c88cf5f3.js"><link rel="prefetch" href="/assets/js/29.3fc7f3c6.js"><link rel="prefetch" href="/assets/js/3.8bb6ac08.js"><link rel="prefetch" href="/assets/js/30.447be628.js"><link rel="prefetch" href="/assets/js/31.39a34521.js"><link rel="prefetch" href="/assets/js/32.325964cd.js"><link rel="prefetch" href="/assets/js/33.939bf5e4.js"><link rel="prefetch" href="/assets/js/34.38fd74ba.js"><link rel="prefetch" href="/assets/js/35.a3664823.js"><link rel="prefetch" href="/assets/js/36.16fca379.js"><link rel="prefetch" href="/assets/js/37.3d72d3a5.js"><link rel="prefetch" href="/assets/js/38.e6e78cb1.js"><link rel="prefetch" href="/assets/js/39.cdd81ca0.js"><link rel="prefetch" href="/assets/js/4.292328b8.js"><link rel="prefetch" href="/assets/js/40.d39a8640.js"><link rel="prefetch" href="/assets/js/41.a7704e18.js"><link rel="prefetch" href="/assets/js/42.6c112f10.js"><link rel="prefetch" href="/assets/js/43.134a79ab.js"><link rel="prefetch" href="/assets/js/45.c513fe1b.js"><link rel="prefetch" href="/assets/js/46.1576c176.js"><link rel="prefetch" href="/assets/js/47.8b7df671.js"><link rel="prefetch" href="/assets/js/48.ca821090.js"><link rel="prefetch" href="/assets/js/49.918455a2.js"><link rel="prefetch" href="/assets/js/5.fc94806e.js"><link rel="prefetch" href="/assets/js/50.5bca0834.js"><link rel="prefetch" href="/assets/js/51.336dfe8a.js"><link rel="prefetch" href="/assets/js/52.492f1671.js"><link rel="prefetch" href="/assets/js/53.77574673.js"><link rel="prefetch" href="/assets/js/54.7a7a4b73.js"><link rel="prefetch" href="/assets/js/55.1b834fdc.js"><link rel="prefetch" href="/assets/js/56.503cfd3a.js"><link rel="prefetch" href="/assets/js/57.f725dd28.js"><link rel="prefetch" href="/assets/js/58.74f9bcfc.js"><link rel="prefetch" href="/assets/js/59.55d9bb25.js"><link rel="prefetch" href="/assets/js/6.89402149.js"><link rel="prefetch" href="/assets/js/60.720b21c1.js"><link rel="prefetch" href="/assets/js/61.00f943a1.js"><link rel="prefetch" href="/assets/js/62.aa45ab1b.js"><link rel="prefetch" href="/assets/js/63.976f7572.js"><link rel="prefetch" href="/assets/js/64.70105afe.js"><link rel="prefetch" href="/assets/js/65.245de16d.js"><link rel="prefetch" href="/assets/js/66.a93d8b13.js"><link rel="prefetch" href="/assets/js/67.92e6af56.js"><link rel="prefetch" href="/assets/js/68.4d31b7ff.js"><link rel="prefetch" href="/assets/js/69.9f292e99.js"><link rel="prefetch" href="/assets/js/7.95301ca9.js"><link rel="prefetch" href="/assets/js/70.6e475ab9.js"><link rel="prefetch" href="/assets/js/71.32bece82.js"><link rel="prefetch" href="/assets/js/72.899aef61.js"><link rel="prefetch" href="/assets/js/73.1a394cb3.js"><link rel="prefetch" href="/assets/js/74.34d59cf0.js"><link rel="prefetch" href="/assets/js/75.3cd99b56.js"><link rel="prefetch" href="/assets/js/76.45c06b6c.js"><link rel="prefetch" href="/assets/js/77.0a09cdd9.js"><link rel="prefetch" href="/assets/js/78.d2ea2a32.js"><link rel="prefetch" href="/assets/js/8.dd5b3f5c.js"><link rel="prefetch" href="/assets/js/9.a2f36449.js">
    <link rel="stylesheet" href="/assets/css/0.styles.6e148dcf.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container no-sidebar"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <!----></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><!---->  <!----> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="object-instance-segmentation-using-tensorflow-framework-and-cloud-gpu-technology"><a href="#object-instance-segmentation-using-tensorflow-framework-and-cloud-gpu-technology" class="header-anchor">#</a> Object Instance Segmentation using TensorFlow Framework and Cloud GPU Technology</h1> <h2 id=""><a href="#" class="header-anchor">#</a> <img src="https://i.ibb.co/SPJWCgc/zzed.jpg" alt=""></h2> <p>In this guide, we will discuss a Computer Vision task: Instance Segmentation. Then, we will present the purpose of this  task  in TensorFlow Framework. Next, we will provide a brief overview of Mask R-CNN network (state-of-the-art model for Instance Segmentation). We also offer a demonstration on Mask R-CNN  model using a jupyter notebook environment: Google Colab</p> <h2 id="what-is-instance-segmentation"><a href="#what-is-instance-segmentation" class="header-anchor">#</a> What is Instance Segmentation?</h2> <p>On the one hand, the Semantic Segmentation (SS) task is one of the Computer Vision task which consists in assigning to each pixel a label among a set of semantic categories.</p> <p><img src="https://i.ibb.co/L6TLXFQ/22E2.jpg" alt=""></p> <p>Ultimately, it is intended to predict a segmentation mask that indicates the category of each pixel. These pixels are classified starting from high-quality feature representations. On the other hand, Instance Segmentation (IS) is based on Semantic Segmentation techniques. It permits to recognize each object instance per pixel for each detected object. These labels are maintained by instance.</p> <p>The common applications and use cases that take place using the Semantic / Instance Segmentation task are the following:</p> <ul><li>Autonomous navigation;</li> <li>Facial Segmentation;</li> <li>Categorizing clothing items;</li> <li>Precision Agriculture.</li> <li>Etc</li></ul> <p>For more details, you can look at two use cases related to Semantic Segmentation challenge:</p> <p>**Use case 1: **<a href="https://blog.playment.io/semantic-segmentation-for-autonomous-vehicles/" target="_blank" rel="noopener noreferrer">Semantic Segmentation for Autonomous vehicles<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <p>**Use case 1: **<a href="https://blog.playment.io/improve-facial-recognition-using-semantic-segmentation-landmark-annotation/" target="_blank" rel="noopener noreferrer">Semantic Segmentation for Facial recognition<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <p>Examples of Instance Segmentation projects and tutorials:</p> <ul><li><p><a href="https://www.pyimagesearch.com/2018/11/26/instance-segmentation-with-opencv/" target="_blank" rel="noopener noreferrer">Instance segmentation with OpenCV<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p></li> <li><p><a href="https://github.com/kulikovv/DeepColoring" target="_blank" rel="noopener noreferrer"> Instance Segmentation by Deep Coloring<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p></li> <li><p><a href="https://medium.com/nanonets/how-to-do-image-segmentation-using-deep-learning-c673cc5862ef" target="_blank" rel="noopener noreferrer">How to do Semantic Segmentation using Deep learning<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p></li></ul> <p>Useful books for learning various aspects of Instance Segmentation:</p> <ol><li>Practical Convolutional Neural Networks: Implement advanced deep learning models using Python: <a href="https://www.amazon.com/Practical-Convolutional-Neural-Networks-Implement/dp/1788392302" target="_blank" rel="noopener noreferrer">Here<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></li> <li>Deep Learning for Computer Vision: <a href="https://" target="_blank" rel="noopener noreferrer">Here<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></li></ol> <h2 id="tensorflow-framework-for-deep-learning"><a href="#tensorflow-framework-for-deep-learning" class="header-anchor">#</a> TensorFlow Framework for Deep Learning</h2> <p><a href="https://www.tensorflow.org/" target="_blank" rel="noopener noreferrer">TensorFlow<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> is an integral open source platform for Machine Learning. It has a scalable and exhaustive environment consisting of tools, libraries and community resources that provide researchers and developers the ability to easily develop and deploy applications based on ML technology. The main features of TensorFlow are illustrated in the Figure below:</p> <p><img src="https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2018/07/Tensorflow-Features.jpg" alt=""></p> <h2 id="prerequisites"><a href="#prerequisites" class="header-anchor">#</a> Prerequisites</h2> <p>Before starting this guide, it is essential to be familiar with the basics of Python programming, Computer Vision concepts, Deep Learning Libraries (TensorFlow + Keras Framework), and OpenCV library.</p> <h2 id="guide-map"><a href="#guide-map" class="header-anchor">#</a> Guide map</h2> <p>The content of this guide will be organized according to the following map:</p> <ul><li>What is Instance Segmentation?;</li> <li>TensorFlow Framework for Deep Learning</li> <li>An overview of Mask R-CNN model for Instance Segmentation;</li> <li>Using Google Colab with GPU (enabled);</li> <li>Mask R-CNN : Demonstration.</li> <li>References.</li></ul> <h2 id="an-overview-of-mask-r-cnn-model-for-instance-segmentation"><a href="#an-overview-of-mask-r-cnn-model-for-instance-segmentation" class="header-anchor">#</a> An overview of Mask R-CNN model for Instance Segmentation</h2> <p>Thanks to Mask R-CNN, we can automatically segment and construct pixel masks for each object in input image. We will apply Mask R-CNN  to  visual data such as images and videos.
Mask R-CNN algorithm was presented by He et al[1]. In fact, It builds on previous object detection works, by R-CNN (2013)[2], Fast R-CNN (2015)[3] and Faster R-CNN (2015)[4] respectively. Mask R-CNN not only generates the bounding box for a detected object, but also generates a predictive mask.</p> <p>Mask R-CNN model is based on Faster R-CNN architecture with 2 major contributions:</p> <ol><li>Replacement of the ROI Pooling module by a more precise module named <em>ROI Align</em>;</li> <li>Inserting an additional branch from the ROI Align module.</li></ol> <p>This additional branch takes the output of the ROI Align and then sends it into two  convolution layers (CONV). The output of the convolution layers (CONV) is the predicted mask itself.
In the following figure, we can see the block diagram  of Mask R-CNN:</p> <p><img src="https://www.pyimagesearch.com/wp-content/uploads/2018/11/mask_rcnn_mask_resizing.jpg" alt="Mask R-CNN"></p> <h2 id="using-google-colab-with-gpu-enabled"><a href="#using-google-colab-with-gpu-enabled" class="header-anchor">#</a> Using Google Colab with GPU (enabled)</h2> <p>Google Colab has been developped to facilitate collaboration between Machine Learning professionals in a more transparent way.</p> <p>Sign in to your Google Gmail account in the top right corner, if you haven't already done so. It
will ask you to open it with Colab at the top of the screen. Then you will make a copy so that you can edit it.</p> <p><img src="https://i.ibb.co/pzH18dw/1.png" alt=""></p> <p>It is now possible to click on &quot;<em>Runtime</em>&quot; menu button to select the Python version and use GPU/CPU device to speed up the calculation.</p> <p><img src="https://i.ibb.co/T1JFqCf/2.png" alt=""></p> <p>Now, everything is ready for the environment.</p> <h3 id="verification-that-tensorflow-is-able-to-detect-the-gpu-device"><a href="#verification-that-tensorflow-is-able-to-detect-the-gpu-device" class="header-anchor">#</a> Verification  that TensorFlow is able to detect the GPU device:</h3> <p>Just select &quot;GPU&quot; from the Notebook Settings Accelerator drop-down menu (via Edit menu or  cmd/ctrl-shift-P command).</p> <p>Execute this psedo-code to confirm that TensorFlow can detect the GPU:</p> <div class="language- extra-class"><pre class="language-text"><code>import tensorflow as tf
device_name = tf.test.gpu_device_name() 
if device_name != '/device:GPU:0':
raise SystemError('GPU device is not detected') 
print('Detected GPU at: {}'.format(device_name))
</code></pre></div><p>It's coming out:</p> <p><code>Found GPU at: /device:GPU:0</code></p> <p>If you are interested in the type of GPU being used. It's a Nvidia Tesla K80 with 24G of memory. Quite powerful.</p> <p>Run this code to find out for yourself.</p> <div class="language- extra-class"><pre class="language-text"><code>from tensorflow.python.client import device_lib 
device_lib.list_local_devices()
</code></pre></div><p>It's coming out:</p> <p><code>physical_device_desc: &quot;device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7&quot;]</code></p> <h2 id="mask-r-cnn-demonstration"><a href="#mask-r-cnn-demonstration" class="header-anchor">#</a> Mask R-CNN : Demonstration</h2> <p>This section provides an implementation of Mask R-CNN on Keras+TensorFlow Framework.</p> <p>###1. Installing dependencies and running the demo</p> <p>Mask R-CNN has some dependencies to install before you can run the demo. Colab allows you to install Python packages via the <code>pip</code> command, and general Linux packaging/libraries via the <code>apt-get</code> command.</p> <p>In case you haven't heard yet. Your current instance of Google Colab runs on an Ubuntu virtual machine. You can execute almost all the Linux commands that you usually do on a Linux machine.
Mask R-CNN depends on <a href="https://pypi.org/project/pycocotools/" target="_blank" rel="noopener noreferrer"><code>pycocotools</code><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> package, you can install it with the following commands:</p> <div class="language- extra-class"><pre class="language-text"><code>!pip install Cython
!git clone https://github.com/waleedka/coco
!pip install -U setuptools
!pip install -U wheel
!make install -C coco/PythonAPI
</code></pre></div><p>It clones GitHub's repository. Install the compilation dependencies. Finally, compile and install the coco API library. All this happens in the cloud virtual machine quite quickly.</p> <p>You are now ready to clone the Mask R-CNN directory of GitHub and access into this directory.</p> <div class="language- extra-class"><pre class="language-text"><code>!git clone https://github.com/matterport/Mask_RCNNN
# cd to the reference directory and possibility to download the pre-trained weight. 
import os
os.chdir('./Mask_RCNN')
!wget https://github.com/matterport/Mask_RCNNN/releases/download/v2.0/mask_rcnn_coco.h5
</code></pre></div><p>Note that you change directories with the Python script instead of executing a <code>cd</code> shell command since you execute Python in the current notebook.</p> <p>Now you can run the demo of Mask R-CNN on Colab, as you would on a local machine.</p> <p>Follow the below Python codes in order to familiarize yourself with the use of a pre-trained model for detecting and segmenting objects. All psedo-codes will be commented on.</p> <div class="language- extra-class"><pre class="language-text"><code>#import of the necessary packages
import os 
import sys 
import random 
import math
import numpy as np 
import skimage.io 
import matplotlib
import matplotlib.pyplot as plt 
import coco
import utils
import visualize
%matplotlib inline
</code></pre></div><div class="language- extra-class"><pre class="language-text"><code># Root directory of the project 
ROOT = os.getcwd()
# Directory to save the trained model and logs files
MODEL= os.path.join(ROOT, &quot;logs&quot;)
# Local path to trained weights file
COCO_MODEL = os.path.join(ROOT, &quot;mask_rcnn_coco.h5&quot;) 
# Download COCO trained weights
if not os.path.exists(COCO_MODEL): utils.download_trained_weights(COCO_MODEL)
# Image directory to be detected
IMAGE = os.path.join(ROOT, &quot;images&quot;)
</code></pre></div><p>###2. Model configurations</p> <p>We will use a model trained on the <a href="http://cocodataset.org/" target="_blank" rel="noopener noreferrer">MS-COCO dataset<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> (It is a large-scale object detection, segmentation, and captioning dataset). The model configurations are in <code>CocoConfig class</code> of coco.py file.</p> <p>Make slight changes to the configurations depending on the task. To do this, subclassify the CocoConfig class and replace its attributes that you need to modify.</p> <div class="language- extra-class"><pre class="language-text"><code>class InferenceConfig(coco.CocoConfig):
# Set the batch size to 1 as we will perform the inference on 1 image at a time. Batch size = GPU_NB * IMAGES_PER_GPU 
GPU_NB = 1
IMAGES_PER_GPU = 1
config = InferenceConfig() 
config.display()
</code></pre></div><p>###3. Building models and importing trained weights</p> <p>In order to create models and load trained weights , please type the following psedo-codes:</p> <div class="language- extra-class"><pre class="language-text"><code># Create model 
model = modellib.MaskRCNN(mode=&quot;inference&quot;, model_dir=MODEL, config=config)

# Load COCO trained weights
model.load_weights(COCO_MODEL, by_name=True)
</code></pre></div><p>###4. Data preparation: MS-COCO dataset</p> <p>The model classifies objects and returns class IDs, which are integer values that identify each class. Some datasets assign integer values to their classes and others do not. For example, in the MS-COCO dataset, the &quot;person&quot; class is 1. IDs are often sequential, but not always. The COCO dataset, for example, has classes associated with class IDs of classes 70 and 72, but not 71.</p> <p>To get the list of class names, you can load the dataset and then use the <code>class_names</code> property like this:</p> <div class="language- extra-class"><pre class="language-text"><code># Loading MS-COCO dataset
dataset = coco.CocoDataset() 
dataset.load_coco(COCO_DIR, &quot;train&quot;)
dataset.prepare()
# Print class names
print(dataset.class_names)
</code></pre></div><p>You have included the list of class names below. The name index of the class in the list represents its ID (first class is 0, second is 1, etc.)</p> <div class="language- extra-class"><pre class="language-text"><code># COCO Class names by indexes
class_names = ['BG','person','bicycle','car','motorcycle','airplane','bus','train','truck','boat','traffic light']
</code></pre></div><p>###5. Starting object detection process</p> <p>To perform object detection, just type the following psedo-codes:</p> <div class="language- extra-class"><pre class="language-text"><code># Loading a random image from the dataset

file_names = next(os.walk(IMAGE))[2]
image = skimage.io.imread(os.path.join(IMAGE, random.choice(file_names)))
# Running object detection
results = model.detect([image], verbose=1)
# Evaluating results 
r = results[0]
visualize.display_instances(image, r['kings'], r['masks'], r['class_ids'],
class_names, r['scores'])
</code></pre></div><p>###6. Customization of images to be segmented</p> <p>You can download an image from a third party website such as:</p> <ul><li><a href="https://imgbbb.com/" target="_blank" rel="noopener noreferrer">Imgbbbb<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></li> <li><a href="https://github.com" target="_blank" rel="noopener noreferrer">GitHub<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></li></ul> <p>You can download your image using <code>wget</code> command.</p> <div class="language- extra-class"><pre class="language-text"><code># Loading a random image from the dataset
file_names = next(os.walk(IMAGE_DIR))[2]
image = skimage.io.imread(os.path.join(IMAGE,'my_image.jpg'))
# Running object detection
results = model.detect([image], verbose=1)
# Evaluating results 
r = results[0]
visualize.display_instances(image, r['kings'], r['masks'], r['class_ids'],
class_names, r['scores'])
</code></pre></div><p>For example, the result of object detection and segmentation is shown below:</p> <p><img src="https://cdn-images-1.medium.com/max/1200/1*8eIcVM-M506P4bA0Y7pbag.png" alt=""></p> <h3 id="_7-video-object-segmentation"><a href="#_7-video-object-segmentation" class="header-anchor">#</a> 7. Video object segmentation</h3> <p>There are 3  steps to processing a video file.</p> <ol><li>Transforming video frames into static images;</li> <li>Image processing;</li> <li>Converting processed images into output videos.</li></ol> <p>In our previous demo, we asked the model to process only 1 image at a time, as configured in <code>IMAGES_PER_GPU</code> option.</p> <div class="language- extra-class"><pre class="language-text"><code>class InferenceConfig(coco.CocoConfig):
#  Set the batch size to 1 as we will perform the inference on 1 image at a time. Batch size = GPU_NB * IMAGES_PER_GPU 
IMAGES_PER_GPU = 1
</code></pre></div><p>If we are going to process all the video at once, it will take a long time. We will therefore use the GPU to operate several frames simultaneously.
The Mask R-CNN pipeline is quite computationally intensive and requires a lot of GPU memory. In Colab, The Tesla K80 GPU with 24G of memory can safely process 3 images at a time. If you go any further, the notebook may crash in the middle of video processing.
Thus, in the psedo-code below, we set the <code>batch_size</code> to 3 and use the <code>cv2 library</code> to take 3 images at a time before processing them with the model.</p> <div class="language- extra-class"><pre class="language-text"><code>capture = cv2.VideoCapture(os.path.join(VIDEO, 'demo.mp4')) 
while True:
ret, frame = capture.read()
# Save each frame of the video to a list
frame_count += 1
frames.append(frame)
if len(frames) == batch_size:
results = model.detect(frames, verbose=0)
for i, item in enumerate(zip(frames, results)): frame = item[0]
r = item[1]
frame = display_instances(
frame, r['kings'], r['masks'], r['class_ids'], class_names, r['scores']
)
name = '{0}.jpg'.format(frame_count + i - batch_size) 
name = os.path.join(VIDEO_SAVE_DIR, name)
cv2.imwrite(name, frame)
# For starting the next batch 
frames = []
</code></pre></div><p>After running this psedo-code, you should now have all the processed image files in <code>./videos/save folder.</code>
The next step is easy, you have to generate the new video from these images. We will use <code>VideoWriter ()</code> function from OpenCV (cv2) to do this.</p> <p>But there are two things you want to be sure of:</p> <p><strong>1. Images must be indexed in the same way</strong></p> <div class="language- extra-class"><pre class="language-text"><code># Get all image file paths.
images = list(glob.iglob(os.path.join(VIDEO_SAVE,' *.*'))
# Sort the images by index.
images = sorted(images, key=lambda x: float(os.path.split(x)[1][:-3]))
</code></pre></div><p><strong>2. The frame rate corresponds to the original video. You can use the following psedo-code to check it or simply open the file property.</strong></p> <div class="language- extra-class"><pre class="language-text"><code>video = cv2.VideoCapture(os.path.join(VIDEO_DIR, trailer1.mp4'));
# Get OpenCV version
(major_ver, minor_ver, subminor_ver) = (cv2. version).split('.'')
if int(major_ver) &lt; 3 :
fps = video.get(cv2.cv.CV_CAP_PROP_FPS)
print(&quot;Frames per second: {0}&quot;.format(fps)) else :
fps = video.get(cv2.CAP_PROP_FPS)
print(&quot;Frames per second: {0}&quot;.format(fps))
video.release();
</code></pre></div><p>Finally, you can use this psedo-code to generate video from the processed images.</p> <div class="language- extra-class"><pre class="language-text"><code>def generate_video(outvid, images=None, fps=30, size=None,is_color=True, format=&quot;FMP4&quot;):
  
  &quot;&quot;&quot;
Create a video from a list of images.
@param outvid output video
@param images list of images to use in the video 
@param fps frame per second
@param size size of each frame 
@param is_color color

&quot;&quot;&quot;
from cv2 import VideoWriter, VideoWriter_fourcc, imread, resize 
fourcc = VideoWriter_fourcc(*format)
vid = None
for image in images:
if not os.path.path.exists(image):
  raise FileNotFoundError(image)
img = imread(image) 
if vid is None:
  if size is None:
size = img.shape[1], img.shape[0]
vid = VideoWriter(outvid, fourcc, float(fps), size, is_color)
  if size[0] != img.shape[1] and size[1] != img.shape[0]:
img = resize(img, size) 
vid.write(img)
vid.release() 
return vid
</code></pre></div><div class="language- extra-class"><pre class="language-text"><code>import glob import os
# Image directory to be detected
ROOT = os.getcwd()
VIDEO = os.path.join(ROOT, &quot;videos&quot;)
VIDEO_SAVE = os.path.join(VIDEO, &quot;save&quot;)
images = list(glob.iglob(os.path.join(VIDEO_SAVE, '*.*'))) 
# Sort the images by index
images = sorted(images, key=lambda x: float(os.path.split(x)[1][:-3]))
outvid = os.path.join(VIDEO, &quot;out_video.mp4&quot;) 
generate_video(outvid, images, fps=30)
</code></pre></div><p>Once this step is completed, the segmented video should now be ready to be downloaded into your local machine.</p> <div class="language- extra-class"><pre class="language-text"><code>from google.colab import files 
files.download('videos/out_video.mp4')
</code></pre></div><p>##List of References:
[1] K. He, G. Gkioxari, P. Dollár, and R. Girshick, &quot;Mask R-CNN&quot;, arXiv:1703.06870[cs], March 2017.</p> <p>[2] R. Girshick, J. Donahue, T. Darrell, et J. Malik, « Rich feature hierarchies for accurate object detection and semantic segmentation », arXiv:1311.2524 [cs], nov. 2013.</p> <p>[3] R. Girshick, &quot;Fast R-CNN&quot;, arXiv:1504.08083[cs], Apr. 2015.</p> <p>[4] S. Ren, K. He, R. Girshick, et J. Sun, « Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks », arXiv:1506.01497 [cs], juin 2015.</p> <p>####Other sources :
TensorFlow, https://www.tensorflow.org/</p> <p>Keras, https://keras.io/</p></div> <footer class="page-edit"><!----> <!----></footer> <!----> </main></div><div class="global-ui"></div></div>
    <script src="/assets/js/app.124452e9.js" defer></script><script src="/assets/js/2.7c027d2f.js" defer></script><script src="/assets/js/44.e37bb31e.js" defer></script>
  </body>
</html>
