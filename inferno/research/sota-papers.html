<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Research papers explained | Virgilio</title>
    <meta name="generator" content="VuePress 1.5.2">
    
    <meta name="description" content="Data Science E-Learning">
    <link rel="preload" href="/Virgilio/assets/css/0.styles.35d5f61b.css" as="style"><link rel="preload" href="/Virgilio/assets/js/app.6448cc99.js" as="script"><link rel="preload" href="/Virgilio/assets/js/2.673d27ad.js" as="script"><link rel="preload" href="/Virgilio/assets/js/13.9e3670c4.js" as="script"><link rel="prefetch" href="/Virgilio/assets/js/10.ba7ac48c.js"><link rel="prefetch" href="/Virgilio/assets/js/11.a9947223.js"><link rel="prefetch" href="/Virgilio/assets/js/12.e76a6842.js"><link rel="prefetch" href="/Virgilio/assets/js/14.aba0c7c5.js"><link rel="prefetch" href="/Virgilio/assets/js/15.e8daac7f.js"><link rel="prefetch" href="/Virgilio/assets/js/16.9d30529b.js"><link rel="prefetch" href="/Virgilio/assets/js/17.bce996fa.js"><link rel="prefetch" href="/Virgilio/assets/js/18.9ff7d58d.js"><link rel="prefetch" href="/Virgilio/assets/js/19.c7e866db.js"><link rel="prefetch" href="/Virgilio/assets/js/20.2b924fc1.js"><link rel="prefetch" href="/Virgilio/assets/js/21.d9a5a534.js"><link rel="prefetch" href="/Virgilio/assets/js/22.2d8c937c.js"><link rel="prefetch" href="/Virgilio/assets/js/23.cd625758.js"><link rel="prefetch" href="/Virgilio/assets/js/24.61dc4cc3.js"><link rel="prefetch" href="/Virgilio/assets/js/25.14296efc.js"><link rel="prefetch" href="/Virgilio/assets/js/26.2108d1b4.js"><link rel="prefetch" href="/Virgilio/assets/js/27.e9acffe2.js"><link rel="prefetch" href="/Virgilio/assets/js/28.111674c6.js"><link rel="prefetch" href="/Virgilio/assets/js/29.4498e30d.js"><link rel="prefetch" href="/Virgilio/assets/js/3.6710cd8e.js"><link rel="prefetch" href="/Virgilio/assets/js/30.d393b764.js"><link rel="prefetch" href="/Virgilio/assets/js/31.46f91a9c.js"><link rel="prefetch" href="/Virgilio/assets/js/32.01d555d5.js"><link rel="prefetch" href="/Virgilio/assets/js/33.8e3e164f.js"><link rel="prefetch" href="/Virgilio/assets/js/34.44d9aebe.js"><link rel="prefetch" href="/Virgilio/assets/js/35.9d0a29a3.js"><link rel="prefetch" href="/Virgilio/assets/js/36.2a48a4f7.js"><link rel="prefetch" href="/Virgilio/assets/js/37.0cf24f97.js"><link rel="prefetch" href="/Virgilio/assets/js/38.e83eef7e.js"><link rel="prefetch" href="/Virgilio/assets/js/39.2000cedb.js"><link rel="prefetch" href="/Virgilio/assets/js/4.dbb1d721.js"><link rel="prefetch" href="/Virgilio/assets/js/40.a88f2e84.js"><link rel="prefetch" href="/Virgilio/assets/js/41.ea1fd39e.js"><link rel="prefetch" href="/Virgilio/assets/js/42.2f178fd6.js"><link rel="prefetch" href="/Virgilio/assets/js/43.3b813986.js"><link rel="prefetch" href="/Virgilio/assets/js/44.e3a5b0d2.js"><link rel="prefetch" href="/Virgilio/assets/js/5.189739e9.js"><link rel="prefetch" href="/Virgilio/assets/js/6.4cfed4ae.js"><link rel="prefetch" href="/Virgilio/assets/js/7.ecd305bb.js"><link rel="prefetch" href="/Virgilio/assets/js/8.cdf6e836.js"><link rel="prefetch" href="/Virgilio/assets/js/9.fc5bd495.js">
    <link rel="stylesheet" href="/Virgilio/assets/css/0.styles.35d5f61b.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/Virgilio/" class="home-link router-link-active"><!----> <span class="site-name">Virgilio <span class="site-name2"> Data Science </span></span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="https://github.com/virgili0/Virgilio" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Contribute
  <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="https://github.com/virgili0/Virgilio" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Contribute
  <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></div> <!----></nav>  <ul class="sidebar-links"><li><a href="/Virgilio/" aria-current="page" class="sidebar-link">What is Virgilio?</a></li><li><section class="sidebar-group depth-0"><p class="sidebar-heading"><span>Paradiso</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/Virgilio/paradiso/demystification-ai-ml-dl.html" class="sidebar-link">Demystification of the key concepts of AI and ML</a></li><li><a href="/Virgilio/paradiso/what-do-i-need-for-ml.html" class="sidebar-link">What do I need to do Machine Learning?</a></li><li><a href="/Virgilio/paradiso/do-you-really-need-ml.html" class="sidebar-link">Do you really need Machine Learning?</a></li><li><a href="/Virgilio/paradiso/use-cases.html" class="sidebar-link">Machine Learning use cases</a></li><li><a href="/Virgilio/paradiso/virgilio-teaching-strategy.html" class="sidebar-link">Virgilio's Teaching Strategy</a></li><li><a href="/Virgilio/paradiso/introduction-to-ml.html" class="sidebar-link">Introduction to Machine Learning</a></li></ul></section></li><li><section class="sidebar-group depth-0"><p class="sidebar-heading"><span>Purgatorio</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><section class="sidebar-group is-sub-group depth-1"><p class="sidebar-heading open"><span>Fundamentals</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/Virgilio/purgatorio/fundamentals/math-fundamentals.html" class="sidebar-link">Linear Algebra</a></li><li><a href="/Virgilio/purgatorio/fundamentals/statistics-fundamentals.html" class="sidebar-link">Statistics</a></li><li><a href="/Virgilio/purgatorio/fundamentals/python-fundamentals.html" class="sidebar-link">Python</a></li><li><a href="/Virgilio/purgatorio/fundamentals/jupyter-notebook.html" class="sidebar-link">Jupyter Notebook</a></li><li><a href="/Virgilio/purgatorio/fundamentals/the-data-science-process.html" class="sidebar-link">The Data Science Process</a></li></ul></section></li><li><section class="sidebar-group is-sub-group depth-1"><p class="sidebar-heading"><span>Define The Scope and Ask Questions</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/Virgilio/purgatorio/define-the-scope-and-ask-questions/frame-the-problem.html" class="sidebar-link">Frame the Problem</a></li><li><a href="/Virgilio/purgatorio/define-the-scope-and-ask-questions/usage-and-integration.html" class="sidebar-link">Usage and Integration</a></li><li><a href="/Virgilio/purgatorio/define-the-scope-and-ask-questions/starting-a-data-project.html" class="sidebar-link">Starting a Data Project</a></li><li><a href="/Virgilio/purgatorio/define-the-scope-and-ask-questions/workspace-setup-and-cloud-computing.html" class="sidebar-link">Workspace Setup and Cloud Computing</a></li></ul></section></li><li><section class="sidebar-group is-sub-group depth-1"><p class="sidebar-heading"><span>Collect and Prepare Data</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/Virgilio/purgatorio/collect-and-prepare-data/data-collection.html" class="sidebar-link">Data Collection</a></li><li><a href="/Virgilio/purgatorio/collect-and-prepare-data/data-preparation.html" class="sidebar-link">Data Preparation</a></li><li><a href="/Virgilio/purgatorio/collect-and-prepare-data/data-visualization.html" class="sidebar-link">Data Visualization</a></li></ul></section></li><li><section class="sidebar-group is-sub-group depth-1"><p class="sidebar-heading"><span>Select and Train Machine Learning Models</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/Virgilio/purgatorio/select-and-train-machine-learning-models/machine-learning-theory.html" class="sidebar-link">Machine Learning Theory</a></li><li><a href="/Virgilio/purgatorio/select-and-train-machine-learning-models/deep-learning-theory.html" class="sidebar-link">Deep Learning Theory</a></li><li><a href="/Virgilio/purgatorio/select-and-train-machine-learning-models/evaluation-and-finetuning.html" class="sidebar-link">Evaluation and Fine Tuning</a></li><li><a href="/Virgilio/purgatorio/select-and-train-machine-learning-models/tools-and-libraries.html" class="sidebar-link">Tools and Libraries</a></li></ul></section></li><li><section class="sidebar-group is-sub-group depth-1"><p class="sidebar-heading"><span>Launch and Mantain the System </span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/Virgilio/purgatorio/launch-and-mantain-the-system/serving-trained-models.html" class="sidebar-link">Serving Trained Models</a></li><li><a href="/Virgilio/purgatorio/launch-and-mantain-the-system/monitoring-usage-and-behavior.html" class="sidebar-link">Monitoring Usage and Behavior</a></li><li><a href="/Virgilio/purgatorio/launch-and-mantain-the-system/automation-and-reproducibility.html" class="sidebar-link">Automation and Reproducibility</a></li></ul></section></li></ul></section></li><li><section class="sidebar-group depth-0"><p class="sidebar-heading open"><span>Inferno</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><section class="sidebar-group is-sub-group depth-1"><p class="sidebar-heading"><span>Computer Vision</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/Virgilio/inferno/computer-vision/introduction-to-computer-vision.html" class="sidebar-link">Introduction to Computer Vision using OpenCV and Python</a></li><li><a href="/Virgilio/inferno/computer-vision/object-instance-segmentation.html" class="sidebar-link">Object Instance Segmentation using TensorFlow Framework and Cloud GPU Technology</a></li><li><a href="/Virgilio/inferno/computer-vision/object-tracking.html" class="sidebar-link">Object Tracking based on Deep Learning</a></li><li><a href="/Virgilio/inferno/computer-vision/Object_detection_based_on_Deep_Learning.html" class="sidebar-link">Object detection based on Deep Learning</a></li></ul></section></li><li><section class="sidebar-group is-sub-group depth-1"><p class="sidebar-heading"><span>Tools</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/Virgilio/inferno/tools/geo-gebra.html" class="sidebar-link">Geo Gebra</a></li><li><a href="/Virgilio/inferno/tools/latex.html" class="sidebar-link">LaTex</a></li><li><a href="/Virgilio/inferno/tools/regex.html" class="sidebar-link">Regex introduction</a></li><li><a href="/Virgilio/inferno/tools/wolfram-alpha.html" class="sidebar-link">Wolfram Alpha</a></li></ul></section></li><li><section class="sidebar-group is-sub-group depth-1"><p class="sidebar-heading open"><span>Research</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/Virgilio/inferno/research/zotero.html" class="sidebar-link">Zotero</a></li><li><a href="/Virgilio/inferno/research/sota-papers.html" aria-current="page" class="active sidebar-link">Research papers explained</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/Virgilio/inferno/research/sota-papers.html#_2019" class="sidebar-link">2019</a></li><li class="sidebar-sub-header"><a href="/Virgilio/inferno/research/sota-papers.html#_2018" class="sidebar-link">2018</a></li><li class="sidebar-sub-header"><a href="/Virgilio/inferno/research/sota-papers.html#_2017" class="sidebar-link">2017</a></li></ul></li></ul></section></li></ul></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="research-papers-explained"><a href="#research-papers-explained" class="header-anchor">#</a> Research papers explained</h1> <h2 id="_2019"><a href="#_2019" class="header-anchor">#</a> 2019</h2> <ul><li><strong>January - <a href="https://arxiv.org/pdf/1901.03798.pdf" target="_blank" rel="noopener noreferrer">3D Pose Estimation<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></strong></li></ul> <p>A pose estimator takes a video as an input, and outputs a figure that corresponds to the pose of the human individuals present in the video.</p> <p>Current difficulties with creating a reliable and real-time 3D pose estimator include the fact that there is little training data, alongside the fact that occlusions must be taken to account. For example, if a particular body part is blocked from view, a pose estimator must still be able to infer its position from the position of the rest of the body.</p> <p>This model outperforms all present models as it creates both 2D and 3D representations of the poses. It uses an initial 2D pose estimation, and then utilises a neural network that converts this 2D estimation into a 3D form. It then uses a 3D-to-2D neural network network to convert the pose back into 2D form, which helps to refine the intermediate 3D pose prediction via a self-supervised correction mechanism that can detect the accuracy of the first 2D-to-3D neural network.</p> <p>The networks allows for the pose estimation to be obtained in about 50 milliseconds, which is nearly 20 frames per second. This is close to real-time, and is suitable for many of the applications of pose estimation.<br></p> <ul><li><strong>February - <a href="https://arxiv.org/pdf/1902.06838.pdf" target="_blank" rel="noopener noreferrer">SC-FEGAN: Face-Editing GAN<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></strong></li></ul> <p>This AI is a able to generate realistic images from a set of controllable patterns. It builds on a couple of previous papers in the field - the first is the paper that generates an image from a sparse description (such as a written sentence), and the second is the paper which allows for facial features on images to be customised (such as merging two different faces).</p> <p>This technique allows us to edit more specific factors - for example, put a smile on someone’s face or remove the sunglasses of an individual. Colour can also be changed - for example, the colour of one’s eye can be manipulated. It is extremely fast, and takes just 50 milliseconds to create these images with 512 x 512 images.</p> <p>It has applications in the editing industry in filmmaking, but can also be used by novel consumers who are looking for simple edits to their photos. Though no web app is currently available, it does have its source code publicly-available.</p> <ul><li><strong>February - <a href="https://www.nature.com/articles/s41467-019-08931-6.pdf" target="_blank" rel="noopener noreferrer">Deep Planning Network (PlaNet)<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></strong></li></ul> <p>Google’s PlaNet AI is intended to learn how to plan a sequence of steps that it must take in order to execute a physical goal - for execute, pole balance or walk like a human. The AI must learn in the same manner as a human would - by looking at the pixels of these images (which requires a visual understanding of the context).</p> <p>The AI uses a sparse reward method, which means that it barely gets feedback with regards to its performance on these tasks. The key difference, however, between this and classical Reinforcement Learning methods is that this AI uses models for its learning. This means that it doesn’t learn every new task from scratch, but rather uses its rudimentary understanding that it has gained from previous activities (such as the nature of gravity), and applies this in future ones. Thus, it has a head-start when learning it a game, making it often 50 times more efficient than techniques that begin with learning from scratch.</p> <p>It significantly outperforms other state-of-the-art AI systems in most tasks, such as a cheetah run or human walk. This agent doesn’t require separate training for each activity, as it intermixes its training. Also, it can use just 5 frames of reference for a particular activity in order to learn it, with equates to approximately a fifth of a second of footage. It can then learn how to continue with this activity over a longer period of time.</p> <ul><li><strong>March - <a href="https://www.nature.com/articles/s41467-019-08931-6.pdf" target="_blank" rel="noopener noreferrer">Humans can decipher adversarial images<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></strong></li></ul> <p>Though recent Convolutional Neural Network systems have surpassed human performance in image detection problems, a problem does remain - simply modifying a pixel or two in the image can cause the system to classify the image as something vastly different. For example, reconfiguring a pixel or two is all it takes for a computer to classify an apple as a car. This ability to ‘fool’ image recognition systems has been criticised as an indication that such systems are unable to interpret images in the same manner as a human would, though a recent paper suggests that this may not be the case.</p> <p>In the paper, a pair of cognitive psychologists showed a group of over 1800 subjects images that had already tricked computers into classifying it under the wrong label. They asked people which of two options the computer predicted the object as being - one option being the computer's real conclusion and the other being a random answer. The subjects chose the same answer as computers 75% of the time, and a remarkable 98% of them tended to answer like the computers did.</p> <p>Next, the researchers gave subjects a choice between the system’s answer and its next-best guess for images it guessed incorrectly. Once again, the subjects again validated the computer's choices - 91 percent of those tested agreed with the system’s decision.</p> <p>The study thus provides a degree of evidence that the apparent flaw with Convolutional Neural Network architectures may not be as bad as many think. It provides a new perspective, along with a new experimental paradigm that can be explored.<br><br></p> <h2 id="_2018"><a href="#_2018" class="header-anchor">#</a> 2018</h2> <ul><li><strong>April - <a href="https://arxiv.org/pdf/1804.02900.pdf" target="_blank" rel="noopener noreferrer">ProGanSR<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></strong></li></ul> <p>To acheive super-resolution, which allows the conversion of low-resolution images to higher-resolution ones, this paper recommends improving the image resolutions through a progressive method. It takes several intermediate steps where the image produced is slightly better than the predecessor, a known as 'curriculum learning'.</p> <p>The paper  uses a GAN rather than simply a CNN. Compared to state-of-the-art models, the images produced using the method proposed in this paper are comprehended with a slightly lower accuracy, however they are produced at 5 times the speed.<br></p> <ul><li><strong>June - <a href="https://arxiv.org/pdf/1806.00451.pdf" target="_blank" rel="noopener noreferrer">Do CIFAR-10 Classifiers Generalize to CIFAR-10?<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></strong></li></ul> <p>The ultimate goal of a Machine Learning model is to predict the output accurately on new, unseen instances. When training a Machine Learning model, it is thus crucial that the test data is not involved in the process of creating the model, as this would introduce bias towards the test set. Unfortunately, we typically have limited access to new data from the same distribution, which results in many researchers today using the test set in place of a validation set. This allows for hyperparameters, such as the learning rate, to be optimised in accordance to the distribution of the selected test set.</p> <p>The research paper proposes a new test set with about 2000 instances which matches the distribution of the test set for the CIFAR-10 dataset, a well-known dataset that many modern image classifier models are tested on. It then evaluates the performance of 30 different modern image classification models. It finds that there is a significant drop from the accuracy in the original test set to the new test set - for instance, VGG and ResNet architectures drop from their well-established 93% accuracy to about 85%. However, the performance of classifiers relative to one another remains more or less constant - thus, the distribution in performance of classifiers can be considered to simply be horizontally shifted.</p> <p>The results cast doubt on the robustness of current classifiers. The classification accuracy of widely used models drops significantly - for example, the accuracy loss of VGG and ResNet corresponds to multiple years of progress on the CIFAR-10 dataset. The distribution shift thus questions to what extent current models truly generalise.<br></p> <ul><li><strong>June - <a href="http://rfpose.csail.mit.edu/" target="_blank" rel="noopener noreferrer">RF-Pose<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></strong></li></ul> <p>The paper provides accurate human pose estimation through walls and occlusions. It leverages the fact that wireless signals in the WiFi frequencies traverse walls and reflect off the human body, and uses a deep neural network approach that parses such radio signals to estimate 2D poses. The pose estimation works well regardless of the lighting conditions, and can also detect multiple humans.</p> <p>In the network, there is a teacher network that looks at the colour image of the wall, and predicts the pose that the human is in. There is also a student network that has the signal as an input, and it learns what the different distributions mean, and how they relate to different human positions and poses. The teacher network shows the student network the correct results, and the student learns how to produce them from radio signals instead of images.</p> <p>Besides being used for motion capture in interactive video games, as well as helping create special effects for movies, pose estimation can also be used to help detect issues with a patient’s posture, track the activity of animals, understanding sign language and pedestrian activity in self-driving cars.<br></p> <ul><li><strong>July - <a href="https://arxiv.org/pdf/1807.01697.pdf" target="_blank" rel="noopener noreferrer">Benchmarking Neural Network Robustness to Corruption &amp; Perturbations<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></strong></li></ul> <p>This paper underlines a method to evaluate the performance of Image Classifiers in terms of their ability to withstand corruptions and perturbations. It creates two datasets - ImageNet-C (for corruptions) and ImageNet-P (for petrurbations) - which help test the robustness of Image Classifiers to such variations, which are common in real-life scenarios.</p> <p>In the context of images, a corruption describes a modification to a base image through distorting its details. The paper utilises 15 different corruption functions on ImageNet mages, each of 5 levels of severity. These corruption functions describe methods including Gaussian Noise, the addition of snow and pixelation.</p> <p>A perturbation describes the distorting of images by varying its appearance through transformative methods. The paper utilises 8 different perturbation functions on ImageNet images, including zoom, tilt and translation.</p> <p>Testing the Classifier with images obtained from the ImageNet-C and ImageNet-P datasets, the paper creates a robustness score regarding a its robustness to both corruption and perturbation by averaging its accuracy over all functions of each type and over all levels of severity.<br></p> <ul><li><strong>July - <a href="https://www.nature.com/articles/s41436-018-0072-y" target="_blank" rel="noopener noreferrer">Phrank<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></strong></li></ul> <p>The algorithm produced automates the most labor-intensive part of genetic diagnosis, that of matching a patient’s genetic sequence and symptoms to a disease described in the scientific literature. Without computer help, this match-up process takes 20 to 40 hours per patient - the process involves the expert looking at a list of around 100 of the patient’s suspicious-looking mutations, making an educated guess about which one might cause disease, checking  scientific literature, and then moving on to the next one. The algorithm developed by Bejerano’s team cuts the time needed by 90 percent.</p> <p>The algorithm’s name, Phrank, a mashup of “phenotype” and “rank,” gives a hint of how it works: it compares a patient’s symptoms and gene data to a medical-literature knowledge base, and then simply generates a ranked list of which rare genetic diseases are most likely to be responsible for the symptoms. Phrank, on average, ranked the true diagnosis 4th on the list of potential diagnoses it generated.<br></p> <ul><li><strong>December - <a href="https://arxiv.org/pdf/1811.10597.pdf" target="_blank" rel="noopener noreferrer">GAN Dissection<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></strong></li></ul> <p>This paper proposes a framework to o visualise and understand GANs at the unit, object, and scene levels. It provides the ability to generate images of a scene, identify the GAN units or neurons that contribute to a particular object that is part of a scene, and then harness these to either activate or deactivate the presence of that particular object. This thus enables us to manipulate images without the need for tools like Photoshop.</p> <p>For example, if we had an image of a church, we could indicate to this framework that we wished to remove the doors present in the image. The framework would thus remove the doors while maintaining the structure of the rest of the image in a suitable manner. We could then add the doors back, if desired. On top of this, we can select a particular region of an image where we wish to add something new - for example, I could add a tree to the right side of the image of a church. The framework understands that the trees have a root in the ground, and thus builds on from the ground up in the specified location. The framework is further able to recognise where additions are not suitable - for example, if we wished to draw a door in the sky, the framework would not accept this request.</p> <p>The framework uses a segmentation network along with a dissection method to identify the individual units of the generator that match meaningful object classes, like trees. It then activates and deactivates the neurons corresponding to each object class when they are modified in the image - for example, inserting a tree activates the neurons in the GAN that corresponds to a tree. What allows this is a key finding in the paper - the same neurons control a specific object class in a variety of contexts, even if the final appearance of the object varies tremendously. The same neurons can switch on the concept of a &quot;door&quot; even if a big stone wall requires a big heavy door facing to the left, or a little hut requires a small curtain door facing to the right.<br></p> <ul><li><strong>December - <a href="https://arxiv.org/pdf/1812.04948.pdf" target="_blank" rel="noopener noreferrer">Style-Based Generator<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></strong></li></ul> <p>This research paper, authored by scientists at processor and graphics card company Nvidia, demonstrates the potential of an alternative generator architecture for generative adversarial networks that borrows from style transfer literature. It allows for specific customisation and control over features within a human face. It has the potential to be applied to other fields, and has thus far been tested successfully on cars and rooms.</p> <p>The generator can combine different aspects of images. For example, if one wished to overlay the gender of one face with the face of another, the generator can do so. The aspects that can be transferred include gender, hair length, pose and the presence of glasses.</p> <p>The parameters of the generator can also be controlled one by one without modifying the core content of an image. For example, the presence of a stubble can be modified.</p> <p>The generator can also perform interpolation. This means that if we have two images A and B, the generator can create intermediate images that map one to another. It can even change the gender in the process. All intermediate images look real too.<br><br></p> <h2 id="_2017"><a href="#_2017" class="header-anchor">#</a> 2017</h2> <ul><li><strong>April - <a href="https://ai.googleblog.com/2017/04/federated-learning-collaborative.html?utm_campaign=the_algorithm.unpaid.engagement&amp;utm_source=hs_email&amp;utm_medium=email&amp;utm_content=70607703&amp;_hsenc=p2ANqtz-9kYBnRclCyRm1_Fweb9tezCh4VeAFJVZTpjvf-fzz2akkq4AGCU5Uhhv-4ApNIZO7vb2ZpigcgT_lU3E_2sF1mtaZzqg&amp;_hsmi=70607705" target="_blank" rel="noopener noreferrer">Federatred Learning<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></strong></li></ul> <p>A big problem being faced by organisations working with developing Machine Learning algorithms and systems today regards privacy - consumers are unwilling to allow their data to be viewed by others, as this data is considered sensitive to them. Google AI's new research on Federated Learning proposes a solution to this.</p> <p>The Federated Learning technique relies on distributed training - it allows for models to be trained independently on a subset of the universal data, and then assembles these independent models into a single, master model.</p> <p>There are a couple of use cases for this to better describe how it functions. Firstly, say medical patients are unwilling to have their health records be sent to other hospitals and organisations who they cannot trust. Federated Learning suggests that each hospital construct its own model using the limited patient data that it has, and then it assembles the models of each hospital into a single, unified model using Google's Federated Averaging algorithm. Secondly, say that we wish to train a predictive keyboard to be uniquely suited to our personal typing patterns on our smartphone. We can use a Federated model, which has been trained and compiled from the predictive patterns of many different users and their data, and then pass in our own personal keyboard typing data to update the model to be better suited to our personal typing habits.</p> <p>Federated Learning technqiues have seen many updates and improvements since, and will certainly remain relevant as AI enters a privacy-centered time in its development.<br></p> <ul><li><strong>September - <a href="https://arxiv.org/pdf/1707.09482.pdf" target="_blank" rel="noopener noreferrer">Deep Feature Consistent Deep Image Transformations (DFC-DIT)<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></strong></li></ul> <p>Say you need to downscale an image of yours without reducing the accuracy of the main features, or if you wished to remove RGB colours from an image, or if you display an image of high dynamic range on a screen that doesn’t support the range. Though there are hundreds of existing structures that do these, this paper describes a method that does these exceptionally well in comparison to current methods.</p> <p>The paper suggests a Deep Feature Consistent Deep Image Transformation (DFC-DIT) framework. It utilises a Convolutional Neural Network (CNN) that produces three outputs for an input image - a downscaled version, a decolorised version and a HDR tone mapped version. It also uses another pretrained and fixed deep CNN that employs the deep feature consistency principle - this ensures that all main features are preserved in the image.<br></p></div> <footer class="page-edit"><!----> <!----></footer> <div class="page-nav"><p class="inner"><span class="prev">
      ←
      <a href="/Virgilio/inferno/research/zotero.html" class="prev">
        Zotero
      </a></span> <!----></p></div> </main></div><div class="global-ui"></div></div>
    <script src="/Virgilio/assets/js/app.6448cc99.js" defer></script><script src="/Virgilio/assets/js/2.673d27ad.js" defer></script><script src="/Virgilio/assets/js/13.9e3670c4.js" defer></script>
  </body>
</html>
